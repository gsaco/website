<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Notes & Resources ‚Äî Gabriel Saco Alvarado</title>
  <meta name="description" content="Technical notes, tutorials, and resources on econometrics, statistics, and programming.">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:wght@400;600;700&family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="assets/css/style.css">
  <!-- Prism.js for syntax highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
  <div class="bg-animated"></div>

  <!-- Navigation -->
  <header class="navbar">
    <div class="container navbar-inner">
      <a href="index.html" class="navbar-brand">
        <span class="brand-icon">GS</span>
        <span>Gabriel Saco</span>
      </a>
      <button class="menu-toggle" aria-label="Toggle menu" aria-expanded="false">
        <span></span><span></span><span></span>
      </button>
      <nav class="navbar-menu">
        <a href="index.html" class="navbar-link">Home</a>
        <a href="research.html" class="navbar-link">Research</a>
        <a href="projects.html" class="navbar-link">Projects</a>
        <a href="coursework.html" class="navbar-link">Coursework</a>
        <a href="cv.html" class="navbar-link">CV</a>
        <a href="notes.html" class="navbar-link active">Notes</a>
        <a href="contact.html" class="navbar-link">Contact</a>
        <button class="theme-toggle" aria-label="Toggle dark mode">üåô</button>
      </nav>
    </div>
  </header>

  <!-- Page Header -->
  <section class="page-header">
    <div class="container">
      <h1 class="reveal">Notes & Resources</h1>
      <p class="lead reveal">Technical notes, tutorials, and explainers on econometrics, statistics, and programming.</p>
    </div>
  </section>

  <!-- Notes Section -->
  <section class="section">
    <div class="container">
      <div class="notes-layout">
        <!-- Sidebar -->
        <aside class="notes-sidebar reveal">
          <div class="card">
            <h4 class="mb-md">üîç Search Notes</h4>
            <div class="search-wrapper">
              <input type="text" class="search-input" id="notes-search" placeholder="Search notes...">
            </div>
          </div>

          <div class="card">
            <h4 class="mb-md">üè∑Ô∏è Topics</h4>
            <div class="tag-cloud" data-filter-container>
              <button class="tag active" data-filter-tag="all">All</button>
              <button class="tag" data-filter-tag="econometrics">Econometrics</button>
              <button class="tag" data-filter-tag="statistics">Statistics</button>
              <button class="tag" data-filter-tag="python">Python</button>
              <button class="tag" data-filter-tag="machine-learning">Machine Learning</button>
              <button class="tag" data-filter-tag="mathematics">Mathematics</button>
            </div>
          </div>

          <div class="card">
            <h4 class="mb-md">üìÖ Recent</h4>
            <ul class="sidebar-list">
              <li><a href="#note-1">GMM Estimators</a></li>
              <li><a href="#note-2">Monte Carlo Methods</a></li>
              <li><a href="#note-3">Gradient Descent</a></li>
            </ul>
          </div>
        </aside>

        <!-- Notes Content -->
        <div class="notes-content stagger">
          
          <!-- Note 1 -->
          <article class="note-card card reveal" id="note-1" data-filter-item data-tags="econometrics statistics">
            <div class="note-header">
              <span class="note-date">Jan 15, 2025</span>
              <div class="card-tags">
                <span class="tag">Econometrics</span>
                <span class="tag">Statistics</span>
              </div>
            </div>
            <h3 class="note-title">Understanding GMM Estimators</h3>
            <p class="note-excerpt">
              The Generalized Method of Moments (GMM) provides a flexible framework for estimation 
              when we have moment conditions but not a full likelihood function. The key insight 
              is to minimize a quadratic form of sample moments.
            </p>
            
            <div class="note-math">
              <p>The GMM estimator minimizes:</p>
              <p class="math-block">$$\hat{\theta}_{GMM} = \arg\min_{\theta} \left[ \frac{1}{n}\sum_{i=1}^n g(X_i, \theta) \right]' W \left[ \frac{1}{n}\sum_{i=1}^n g(X_i, \theta) \right]$$</p>
              <p>where $g(X_i, \theta)$ are the moment conditions and $W$ is a positive definite weighting matrix.</p>
            </div>

            <details class="note-details">
              <summary>üìñ Read more</summary>
              <div class="note-body">
                <h4>Optimal Weighting Matrix</h4>
                <p>The efficient GMM estimator uses the optimal weighting matrix:</p>
                <p class="math-block">$$W^* = \left[ E[g(X, \theta_0) g(X, \theta_0)'] \right]^{-1}$$</p>
                
                <h4>Python Implementation</h4>
                <pre><code class="language-python">import numpy as np
from scipy.optimize import minimize

def gmm_objective(theta, data, W):
    """GMM objective function."""
    moments = compute_moments(data, theta)
    g_bar = moments.mean(axis=0)
    return g_bar @ W @ g_bar

def gmm_estimate(data, theta_init, moment_func):
    """Two-step GMM estimator."""
    # Step 1: Identity weighting matrix
    W1 = np.eye(len(theta_init))
    res1 = minimize(gmm_objective, theta_init, args=(data, W1))
    
    # Step 2: Optimal weighting matrix
    moments = compute_moments(data, res1.x)
    W2 = np.linalg.inv(moments.T @ moments / len(data))
    res2 = minimize(gmm_objective, res1.x, args=(data, W2))
    
    return res2.x</code></pre>
              </div>
            </details>
          </article>

          <!-- Note 2 -->
          <article class="note-card card reveal" id="note-2" data-filter-item data-tags="statistics python">
            <div class="note-header">
              <span class="note-date">Jan 10, 2025</span>
              <div class="card-tags">
                <span class="tag">Statistics</span>
                <span class="tag">Python</span>
              </div>
            </div>
            <h3 class="note-title">Monte Carlo Methods for Integration</h3>
            <p class="note-excerpt">
              Monte Carlo integration is a powerful technique for approximating integrals, especially 
              in high dimensions where deterministic methods become computationally infeasible.
            </p>
            
            <div class="note-math">
              <p>The fundamental idea: approximate $I = \int_a^b f(x) dx$ using:</p>
              <p class="math-block">$$\hat{I}_n = \frac{b-a}{n} \sum_{i=1}^n f(X_i), \quad X_i \sim \text{Uniform}(a, b)$$</p>
              <p>By the Law of Large Numbers, $\hat{I}_n \xrightarrow{p} I$ as $n \to \infty$.</p>
            </div>

            <details class="note-details">
              <summary>üìñ Read more</summary>
              <div class="note-body">
                <h4>Variance Reduction: Importance Sampling</h4>
                <p>When the integrand has regions of high variance, we can use importance sampling:</p>
                <p class="math-block">$$\hat{I}_{IS} = \frac{1}{n} \sum_{i=1}^n \frac{f(X_i)}{g(X_i)}, \quad X_i \sim g$$</p>
                
                <h4>Python Example</h4>
                <pre><code class="language-python">import numpy as np

def monte_carlo_integrate(f, a, b, n=10000):
    """Basic Monte Carlo integration."""
    x = np.random.uniform(a, b, n)
    return (b - a) * np.mean(f(x))

def importance_sampling(f, proposal_sampler, proposal_pdf, n=10000):
    """Monte Carlo with importance sampling."""
    x = proposal_sampler(n)
    weights = f(x) / proposal_pdf(x)
    return np.mean(weights)

# Example: Integrate exp(-x^2) from 0 to infinity
f = lambda x: np.exp(-x**2)
result = monte_carlo_integrate(f, 0, 5, n=100000)
print(f"Estimate: {result:.6f}")  # True value: sqrt(pi)/2 ‚âà 0.8862</code></pre>
              </div>
            </details>
          </article>

          <!-- Note 3 -->
          <article class="note-card card reveal" id="note-3" data-filter-item data-tags="machine-learning python mathematics">
            <div class="note-header">
              <span class="note-date">Jan 5, 2025</span>
              <div class="card-tags">
                <span class="tag">Machine Learning</span>
                <span class="tag">Python</span>
                <span class="tag">Mathematics</span>
              </div>
            </div>
            <h3 class="note-title">Gradient Descent: From Theory to Practice</h3>
            <p class="note-excerpt">
              Gradient descent is the workhorse of modern machine learning. Understanding its 
              convergence properties and variants is essential for effective model training.
            </p>
            
            <div class="note-math">
              <p>The update rule for minimizing $f(\theta)$:</p>
              <p class="math-block">$$\theta_{t+1} = \theta_t - \eta \nabla f(\theta_t)$$</p>
              <p>For convex $f$ with $L$-Lipschitz gradients and $\eta < \frac{2}{L}$, we have convergence.</p>
            </div>

            <details class="note-details">
              <summary>üìñ Read more</summary>
              <div class="note-body">
                <h4>Stochastic Gradient Descent</h4>
                <p>When $f(\theta) = \frac{1}{n}\sum_{i=1}^n f_i(\theta)$, use unbiased gradient estimates:</p>
                <p class="math-block">$$\theta_{t+1} = \theta_t - \eta_t \nabla f_{i_t}(\theta_t)$$</p>
                
                <h4>Adam Optimizer</h4>
                <pre><code class="language-python">import numpy as np

class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = None
        self.v = None
        self.t = 0
    
    def step(self, params, grads):
        if self.m is None:
            self.m = np.zeros_like(params)
            self.v = np.zeros_like(params)
        
        self.t += 1
        self.m = self.beta1 * self.m + (1 - self.beta1) * grads
        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2
        
        m_hat = self.m / (1 - self.beta1**self.t)
        v_hat = self.v / (1 - self.beta2**self.t)
        
        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)</code></pre>
              </div>
            </details>
          </article>

          <!-- Note 4 -->
          <article class="note-card card reveal" id="note-4" data-filter-item data-tags="econometrics">
            <div class="note-header">
              <span class="note-date">Dec 20, 2024</span>
              <div class="card-tags">
                <span class="tag">Econometrics</span>
              </div>
            </div>
            <h3 class="note-title">Instrumental Variables: The Basics</h3>
            <p class="note-excerpt">
              Instrumental variables (IV) estimation is a fundamental technique for addressing 
              endogeneity in econometric models. An instrument $Z$ must satisfy relevance and 
              exogeneity conditions.
            </p>
            
            <div class="note-math">
              <p>For the model $Y = X\beta + \epsilon$ with endogenous $X$, the IV estimator is:</p>
              <p class="math-block">$$\hat{\beta}_{IV} = (Z'X)^{-1}Z'Y$$</p>
              <p>Requires: $\text{Cov}(Z, X) \neq 0$ (relevance) and $\text{Cov}(Z, \epsilon) = 0$ (exogeneity).</p>
            </div>

            <details class="note-details">
              <summary>üìñ Read more</summary>
              <div class="note-body">
                <h4>Two-Stage Least Squares (2SLS)</h4>
                <p>With multiple instruments, 2SLS provides the efficient estimator:</p>
                <ol>
                  <li><strong>First stage:</strong> Regress $X$ on $Z$: $\hat{X} = Z(Z'Z)^{-1}Z'X$</li>
                  <li><strong>Second stage:</strong> Regress $Y$ on $\hat{X}$: $\hat{\beta}_{2SLS} = (\hat{X}'\hat{X})^{-1}\hat{X}'Y$</li>
                </ol>
                <p>Coming soon: Python implementation with weak instrument diagnostics.</p>
              </div>
            </details>
          </article>

          <!-- Placeholder for more notes -->
          <div class="card reveal" style="text-align: center; padding: var(--spacing-xl);">
            <p class="text-muted">More notes coming soon! üìù</p>
            <p class="text-sm text-muted">Topics in progress: Panel Data Methods, Bootstrap Inference, Neural Networks for Economists</p>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <div class="footer-brand">
          <div class="footer-brand-name">Gabriel Saco Alvarado</div>
          <p>Undergraduate economist and aspiring researcher.</p>
          <div class="footer-social">
            <a href="#" aria-label="GitHub">üì¶</a>
            <a href="#" aria-label="LinkedIn">üíº</a>
            <a href="mailto:ga.sacoa@alum.up.edu.pe" aria-label="Email">üìß</a>
          </div>
        </div>
        <div>
          <div class="footer-heading">Navigation</div>
          <ul class="footer-links">
            <li><a href="index.html">Home</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="projects.html">Projects</a></li>
            <li><a href="cv.html">CV</a></li>
          </ul>
        </div>
        <div>
          <div class="footer-heading">Resources</div>
          <ul class="footer-links">
            <li><a href="coursework.html">Coursework</a></li>
            <li><a href="notes.html">Notes</a></li>
            <li><a href="contact.html">Contact</a></li>
          </ul>
        </div>
        <div>
          <div class="footer-heading">Contact</div>
          <ul class="footer-links">
            <li>Lima, Peru</li>
            <li><a href="mailto:ga.sacoa@alum.up.edu.pe">ga.sacoa@alum.up.edu.pe</a></li>
          </ul>
        </div>
      </div>
      <div class="footer-bottom">
        <p>&copy; 2025 Gabriel Saco Alvarado.</p>
      </div>
    </div>
  </footer>

  <button class="back-to-top" aria-label="Back to top">‚Üë</button>

  <!-- Scripts -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <!-- MathJax for LaTeX -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <script src="assets/js/main.js"></script>
  <script>
    // Notes search functionality
    const notesSearch = document.getElementById('notes-search');
    if (notesSearch) {
      notesSearch.addEventListener('input', (e) => {
        const query = e.target.value.toLowerCase();
        const notes = document.querySelectorAll('.note-card');
        
        notes.forEach(note => {
          const title = note.querySelector('.note-title').textContent.toLowerCase();
          const excerpt = note.querySelector('.note-excerpt').textContent.toLowerCase();
          const matches = title.includes(query) || excerpt.includes(query);
          
          note.style.display = matches ? '' : 'none';
        });
      });
    }
  </script>
</body>
</html>
